{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jet08013/GitHub/Math-5800-Spring-2020\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "%cd /home/jet08013/GitHub/Math-5800-Spring-2020/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((.5,),(.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST('./data',download=True, train=True,transform=transform)\n",
    "testset = torchvision.datasets.MNIST('./data',download=True,train=False,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (flatten): Flatten()\n",
       "  (linear): Linear(in_features=784, out_features=10, bias=False)\n",
       "  (soft): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(784,10,bias=False)\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.soft(self.linear(self.flatten(x)))\n",
    "\n",
    "LR = Net()\n",
    "LR.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(LR.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.765\n",
      "[1,  4000] loss: 1.600\n",
      "[1,  6000] loss: 1.595\n",
      "[1,  8000] loss: 1.588\n",
      "[1, 10000] loss: 1.584\n",
      "[1, 12000] loss: 1.575\n",
      "[1, 14000] loss: 1.572\n",
      "[2,  2000] loss: 1.569\n",
      "[2,  4000] loss: 1.571\n",
      "[2,  6000] loss: 1.563\n",
      "[2,  8000] loss: 1.561\n",
      "[2, 10000] loss: 1.560\n",
      "[2, 12000] loss: 1.567\n",
      "[2, 14000] loss: 1.560\n",
      "[3,  2000] loss: 1.557\n",
      "[3,  4000] loss: 1.557\n",
      "[3,  6000] loss: 1.553\n",
      "[3,  8000] loss: 1.554\n",
      "[3, 10000] loss: 1.558\n",
      "[3, 12000] loss: 1.565\n",
      "[3, 14000] loss: 1.557\n",
      "[4,  2000] loss: 1.559\n",
      "[4,  4000] loss: 1.557\n",
      "[4,  6000] loss: 1.554\n",
      "[4,  8000] loss: 1.549\n",
      "[4, 10000] loss: 1.553\n",
      "[4, 12000] loss: 1.555\n",
      "[4, 14000] loss: 1.556\n",
      "[5,  2000] loss: 1.553\n",
      "[5,  4000] loss: 1.558\n",
      "[5,  6000] loss: 1.552\n",
      "[5,  8000] loss: 1.549\n",
      "[5, 10000] loss: 1.550\n",
      "[5, 12000] loss: 1.547\n",
      "[5, 14000] loss: 1.551\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # initialize gradient calculation - we will do a batch of 4\n",
    "        outputs = LR(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[{:d}, {:5d}] loss: {:.3f}'.format(epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9045\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = LR(images)\n",
    "        _, pred = torch.max(outputs.data,1)\n",
    "        total+=labels.size(0)\n",
    "        correct += (pred==labels).sum().item()\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=10, bias=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict([('weight', Parameter containing:\n",
       "               tensor([[-0.0247, -0.0433, -0.0248,  ..., -0.0034,  0.0008, -0.0476],\n",
       "                       [ 0.1176,  0.0948,  0.1090,  ...,  0.0775,  0.1325,  0.0856],\n",
       "                       [-0.0578, -0.0721, -0.0433,  ..., -0.0575, -0.0565, -0.0295],\n",
       "                       ...,\n",
       "                       [ 0.0535,  0.0316,  0.0174,  ...,  0.0042,  0.0065,  0.0024],\n",
       "                       [-0.0859, -0.0835, -0.0891,  ..., -0.0913, -0.0544, -0.0726],\n",
       "                       [-0.0125,  0.0039,  0.0108,  ..., -0.0028, -0.0196, -0.0118]],\n",
       "                      device='cuda:0', requires_grad=True)),\n",
       "              ('bias', None)]),\n",
       " '_buffers': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'in_features': 784,\n",
       " 'out_features': 10}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(LR.linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = LR.linear._parameters['weight'].cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [filters[i,:].reshape(28,28) for i in range(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-30d24a0ef781>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2,5)\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(10):\n",
    "    axes[i//5,i%5].imshow(images[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (flatten): Flatten()\n",
      "  (linear): Linear(in_features=784, out_features=200, bias=False)\n",
      "  (hidden): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#No Softmax layer \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(784,200,bias=False)\n",
    "        self.hidden = nn.Linear(200,10,bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        flat = self.flatten(x)\n",
    "        lin = self.linear(flat)\n",
    "        hid = self.hidden(F.relu(lin))\n",
    "        return hid\n",
    "\n",
    "MLP = Net()\n",
    "MLP.to(device)\n",
    "print(MLP)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(MLP.parameters(), lr=.001)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # initialize gradient calculation - we will do a batch of 4\n",
    "        outputs = MLP(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[{:d}, {:5d}] loss: {:.3f}'.format(epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9569\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = MLP(images)\n",
    "        _, pred = torch.max(outputs.data,1)\n",
    "        total+=labels.size(0)\n",
    "        correct += (pred==labels).sum().item()\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (flatten): Flatten()\n",
      "  (linear): Linear(in_features=784, out_features=200, bias=False)\n",
      "  (extra): Linear(in_features=200, out_features=100, bias=False)\n",
      "  (hidden): Linear(in_features=100, out_features=10, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#No Softmax layer \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(784,200,bias=False)\n",
    "        self.extra = nn.Linear(200,100,bias=False)\n",
    "        self.hidden = nn.Linear(100,10,bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        flat = self.flatten(x)\n",
    "        lin = self.linear(flat)\n",
    "        extra = self.extra(F.relu(lin))\n",
    "        hid = self.hidden(F.relu(extra))\n",
    "        return hid\n",
    "\n",
    "MMLP = Net()\n",
    "MMLP.to(device)\n",
    "print(MMLP)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.170\n",
      "[1,  4000] loss: 0.163\n",
      "[1,  6000] loss: 0.179\n",
      "[1,  8000] loss: 0.145\n",
      "[1, 10000] loss: 0.152\n",
      "[1, 12000] loss: 0.148\n",
      "[1, 14000] loss: 0.150\n",
      "[2,  2000] loss: 0.143\n",
      "[2,  4000] loss: 0.125\n",
      "[2,  6000] loss: 0.144\n",
      "[2,  8000] loss: 0.132\n",
      "[2, 10000] loss: 0.146\n",
      "[2, 12000] loss: 0.155\n",
      "[2, 14000] loss: 0.143\n",
      "[3,  2000] loss: 0.126\n",
      "[3,  4000] loss: 0.126\n",
      "[3,  6000] loss: 0.136\n",
      "[3,  8000] loss: 0.120\n",
      "[3, 10000] loss: 0.143\n",
      "[3, 12000] loss: 0.138\n",
      "[3, 14000] loss: 0.147\n",
      "[4,  2000] loss: 0.119\n",
      "[4,  4000] loss: 0.123\n",
      "[4,  6000] loss: 0.147\n",
      "[4,  8000] loss: 0.129\n",
      "[4, 10000] loss: 0.137\n",
      "[4, 12000] loss: 0.132\n",
      "[4, 14000] loss: 0.142\n",
      "[5,  2000] loss: 0.112\n",
      "[5,  4000] loss: 0.106\n",
      "[5,  6000] loss: 0.114\n",
      "[5,  8000] loss: 0.121\n",
      "[5, 10000] loss: 0.124\n",
      "[5, 12000] loss: 0.132\n",
      "[5, 14000] loss: 0.134\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(MMLP.parameters(), lr=.001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # initialize gradient calculation - we will do a batch of 4\n",
    "        outputs = MMLP(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[{:d}, {:5d}] loss: {:.3f}'.format(epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9567\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = MMLP(images)\n",
    "        _, pred = torch.max(outputs.data,1)\n",
    "        total+=labels.size(0)\n",
    "        correct += (pred==labels).sum().item()\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
