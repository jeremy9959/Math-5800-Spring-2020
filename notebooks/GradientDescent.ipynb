{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import Range1d\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "- See [Why Momentum Really Works](https://distill.pub/2017/momentum/) for a beautiful explanation of gradient descent and momentum.  These notes are just my interpretation of the first section of that paper.  Later sections come later....\n",
    "\n",
    "We are given a smooth function $f:\\mathbf{R}^{N}\\to \\mathbf{R}$ and our objective is to find a minimum of this function.  The strategy is to exploit this theorem from multivariate Calculus.\n",
    "\n",
    "**Theorem**: The gradient $\\nabla f$ is a vector field that, at each point $x\\in\\mathbf{R}^{N}$, points in the direction\n",
    "of most rapid increase in $f$; and $-\\nabla f$ points in the direction of most rapid decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our strategy for gradient descent is:\n",
    "\n",
    "**Gradient Descent:** Let $\\alpha$ be a positive real number, and choose $w_{0}\\in\\mathbf{R}^{N}$ \"at random\" as a starting place.  Iteratively define\n",
    "$$\n",
    "w_{n+1} = w_{n} - \\alpha\\nabla f(w_{n}).\n",
    "$$\n",
    "Then the sequence $w_{n}$ converges to a (local) minimum of $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2\n",
    "def Df(x):\n",
    "    return 2*x\n",
    "alpha=.1\n",
    "N=10\n",
    "x=np.zeros(N)\n",
    "x[0]=3\n",
    "for i in range(1,N):\n",
    "    x[i] = x[i-1]-alpha*Df(x[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f = figure(title='Gradient Descent Illustration f(x)=x**2',width=300,height=300)\n",
    "f.xaxis.axis_label='Step Number'\n",
    "f.yaxis.axis_label='Value of x_n'\n",
    "f.y_range=Range1d(0,3)\n",
    "f.line(range(N),x)\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider $f(x)=x^2$.  We can make this very explicit in this case:\n",
    "- the iteration is $x_{n+1}=x_{n}-2\\alpha x_{n} = (1-2\\alpha)x_{n}$.\n",
    "- so $x_{n}=(1-2\\alpha)^{n}x_{0}$\n",
    "As long as the learning rate satisfies $0\\le \\alpha\\le .5$ this converges exponentially to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Following the article cited above, let's look at the problem of finding the minimum of a quadratic function.  This is a common situation in the first place -- think\n",
    "about least squares regression, for example -- but in the neighborhood of a critical point a function is approximately quadratic so the quadratic case sheds light on the general situation as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $A$ be a symmetric invertible $N\\times N$ matrix and let $b$ be a vector in $\\mathbf{R}^{N}$. Define\n",
    "$$\n",
    "f(w) = \\frac{1}{2} w^{\\intercal}Aw -b^{\\intercal}w.\n",
    "$$\n",
    "This is exactly the type of function that appears in a least squares regression problem.  More precisely, suppose we have data points\n",
    "$$\n",
    "(y_1,x_{11},x_{12},\\ldots, x_{1k}), (y_2, x_{21},\\ldots, x_{2k}), \\ldots, (y_{N},x_{N1},\\ldots, x_{Nk})\n",
    "$$\n",
    "and we want to fit a (multi-)linear function to this data to estimate $y$ as a function of the $x_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assemble the data $x_{ij}$ into a matrix $X$ and\n",
    "the points $y_i$ into a vector $Y$.  Our goal is to find a $k\\times 1$ vector $w$ so that\n",
    "$$\n",
    "E=\\|Y-Xw\\|^2\n",
    "$$\n",
    "is minimal.  But \n",
    "$$\\begin{aligned}\n",
    "E(w) &= (Y^{\\intercal}-w^{\\intercal}X^{\\intercal})(Y-Xw) \\\\\n",
    "  &= Y^{\\intercal}Y - Y^{\\intercal}Xw - w^{\\intercal}X^{\\intercal}Y + w^{\\intercal}X^{\\intercal}Xw\\\\\n",
    "  \\end{aligned}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each term in this sum is a $1\\times 1$ matrix and:\n",
    "\n",
    "- $Y^{\\intercal}Y$ is a constant\n",
    "- $Y^{\\intercal}Xw = w^{\\intercal}X^{\\intercal}Y$ since the transpose of a $1x1$ matrix is the matrix itself.\n",
    "\n",
    "As a minimization problem, we can forget about the constant, set $b=X^{\\intercal}Y$, and  minimize the function \n",
    "$$\n",
    "f(w) = -b^{\\intercal}w + \\frac{1}{2}w^{\\intercal}Aw\n",
    "$$\n",
    "where $A=X^{\\intercal}X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Proposition:** The gradient $\\nabla f$ of the function $f(w)$ is $\\nabla f = Aw-b$.\n",
    "\n",
    "**Proof:**  First,\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_{i}} b^{\\intercal}w = b_{i}\n",
    "$$\n",
    "so assembling these we see that the gradient of the first term is indeed $-b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will skip the computation for the second part which is straightforward if you keep your summations well-organized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Corollary:** The minimum occurs at $w_*=A^{-1}b$.\n",
    "\n",
    "The gradient descent iteration then becomes\n",
    "$$\n",
    "w_{n+1} = w_{n} - \\alpha( Aw_{n} -b).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From linear algebra, we can find $Q$ so that $A=Q\\Lambda Q^{\\intercal}$ where $\\Lambda$ is the diagonal matrix of eigenvalues that\n",
    "we assume ordered from smallest $\\lambda_1$ to largest $\\lambda_n$.\n",
    "We set $x=Q^{\\intercal}(w-w_*)$, then\n",
    "the coordinates $x_i$ satisfy:\n",
    "- the center of our coordinate system is at the minimum $A^{-1}b$, and \n",
    "- the coordinate axes are the eigenvectors of the matrix $A$.\n",
    "\n",
    "Also, $Q$ is an orthogonal matrix with $QQ^{T}=I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With this change of coordinates, the gradient descent process \"diagonalizes\" and becomes\n",
    "$$\n",
    "x_{n+1} = (1-\\alpha\\Lambda)x_{n} = (1-\\alpha\\Lambda)^{n}x_{0}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "f(w_{n})-f(w_{*}) = \\frac{1}{2}x_{n}^{\\intercal}\\Lambda x_{n}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The thing to observe here is that the different components converge at different rates, corresponding to the different $(1-\\alpha\\lambda_{i}).$ and\n",
    "(dropping the factor of $1/2$) the error behaves like\n",
    "\n",
    "$$\n",
    "(1-\\alpha\\Lambda)^{2n}(x_{0}^{\\intercal}\\Lambda x_{0}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The norm of the error is\n",
    "\n",
    "\n",
    "$$\n",
    "E_{k} = \\sum_{i=1}^{N} (1-\\alpha\\lambda_{i})^{2k}\\lambda_{i}(x_{0}^{(i)})^2\n",
    "$$\n",
    "\n",
    "where $x_{0}^{(i)}$ is the $i^{th}$ component of the initial guess in the $x$-coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An example\n",
    "\n",
    "Let's look at an example.  For simplicity, we will start with a diagonal matrix.   The argument above shows that this captures the general case. \n",
    "For concreteness, let's have eigenvalues $[.2,2,5]$.  The function we are trying to minimze\n",
    "by gradient descent is just the quadratic form $f(x_1,x_2,x_3) = .2x_1^2+2x_2^2+5x_3^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we choose a step size/learning rate of $\\alpha$, then the iteration we are interested in is\n",
    "$$\n",
    "x_i^{(n)} = (1-\\alpha\\lambda_i)^n x_{i}^{0}\n",
    "$$\n",
    "and the error -- which in this case is just norm of the value of the function -- at the nth stage is\n",
    "$$\n",
    "E_n = \\sum_{i=1}^{3}(1-\\alpha\\lambda_i)^{2n}\\lambda_i (x_i^{0})^2\n",
    "$$\n",
    "where $x_i^{(0)}$ is our initial guess -- let's use $(1,1,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data(alpha, steps):\n",
    "    colors=['blue','green','black']\n",
    "    f=figure(width=400,height=400)\n",
    "    mark=False\n",
    "    Nsave=np.inf\n",
    "    f.y_range=Range1d(0,1)\n",
    "    data = np.zeros(shape=(3,steps))\n",
    "    data[:,0] = np.array([1,1,1])\n",
    "    Lambda = np.diag([.2,2,5])\n",
    "    E = np.zeros(shape=(1,steps))\n",
    "    E[:,0] = np.dot(np.dot(data[:,0],Lambda),data[:,0])\n",
    "    Z = np.identity(3) - alpha*Lambda\n",
    "    for i in range(1,steps):\n",
    "        data[:,i] = np.dot(Z, data[:,i-1])\n",
    "        E[:,i] = np.dot(np.dot(data[:,i],Lambda),data[:,i])\n",
    "        if np.abs(E[:,i])<.01 and mark==False:\n",
    "            Nsave = i\n",
    "            mark=True\n",
    "    for j in range(3):\n",
    "        f.line(range(steps),data[j,:],color=colors[j],legend_label='component {}'.format(j))\n",
    "    f.line(range(steps),E[0,:],color='red',legend_label='Error',line_width=3,line_dash='dotted')\n",
    "    f.title.text = 'Alpha = {}, Steps to convergence = {}'.format(alpha,Nsave)\n",
    "    f.legend.background_fill_alpha=0\n",
    "    f.legend.location='center_right'\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "show(generate_data(.01,1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our best choice of $\\alpha$ has to make $(1-\\alpha\\lambda_i)^n$ go to zero  as fast as possible.  So certainly we need\n",
    "$|(1-\\alpha\\lambda_i)|<1$ or $0<\\alpha\\lambda_i<2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The problem becomes:  Choose $\\alpha$ to simultaneously make all of $|(1-\\alpha\\lambda_{i})|$ as small as possible. Since\n",
    "$0<\\alpha\\lambda_n<2$ we have $0<\\alpha<2/\\lambda_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x=np.linspace(0,2/5,100)\n",
    "e1 = np.abs(1-.2*x)\n",
    "e2 = np.abs(1 -2*x)\n",
    "e3 = np.abs(1 - 5*x)\n",
    "f=figure(width=400,height=400)\n",
    "f.line(x,e1,legend_label='lambda=.2',color='blue')\n",
    "f.line(x,e2,legend_label='lambda=2',color='green')\n",
    "f.line(x,e3,legend_label='lambda=5',color='black')\n",
    "f.legend.background_fill_alpha=0\n",
    "f.legend.location='bottom_left'\n",
    "f.scatter(x=[2/5.2],y=[np.abs(1-(2/5.2)*5)],size=10,color='red')\n",
    "f.title.text=\"Red Dot indicates optimum\"\n",
    "f.xaxis.axis_label='alpha'\n",
    "f.yaxis.axis_label='1-alpha*lambda'\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At the extremes, we have $1-\\alpha\\lambda_1$ and $1-\\alpha\\lambda_n$.\n",
    "\n",
    "Our compromise is to make $1-\\alpha\\lambda_1 = -(1-\\alpha\\lambda_n)$.  This makes the two extremes go to zero at the same rate, and\n",
    "everything in the middle go to zero at least as fast. This means that the optimum $\\alpha$ is\n",
    "\n",
    "$$\n",
    "\\alpha_* = \\frac{2}{(\\lambda_1+\\lambda_n)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "show(generate_data(2/(.2+5),100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The ratio $\\kappa=\\lambda_n/\\lambda_1$ is called the **condition number** of the matrix that we are studying.  The optimal learning rate\n",
    "is $\\alpha = 2/(\\lambda_1+\\lambda_n)$ and the associated (multiplier) rate is:\n",
    "$$\n",
    "(1-\\alpha\\lambda_1) = (1-2\\lambda_1/(\\lambda_1+\\lambda_n)) = (\\lambda_n-\\lambda_1)/(\\lambda_n+\\lambda_1) = (\\kappa-1)/(\\kappa+1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If $\\kappa$ is large, this is close to $1$ and convergence is slow. If $\\kappa$ is just a bit larger than $1$, then this is small and convergence is fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
