{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of:\n",
    "\n",
    "- A collection D of N d-dimensional column vectors x (so an \n",
    "    (d,N) data matrix) \n",
    "    \n",
    "- A label for each column vector between 0 and k-1 which we represent as an \n",
    "    (k,N)\n",
    "    matrix $L$ such that the jth column has a 1 in the ith row if D[:,j] is in class i,\n",
    "    and zeroes elsewhere in that column.\n",
    "\n",
    "The logistic regression model has for parameters a matrix W of size (k,d)\n",
    "and a bias vector b of\n",
    "size (k,1). To simplify the notation, let's add a dimension to our column\n",
    "vectors and set every last value to 1; then we can treat the biases\n",
    "as part of our matrices W.  From now on our dimension d includes this extra\n",
    "dimension.\n",
    "\n",
    "\n",
    "Let $\\sigma$ be the softmax function.  The model assigns the multinomial probabilities\n",
    "$\\sigma(Wx)$ to x and we wish to maximize the likelihood of the data given W.  Note that $\\sigma(Wx)$  is a (k,1) column vector. \n",
    "\n",
    "The relevant loss function is the log-likelihood of the model.  Suppose\n",
    "that the point $D[:,j]$ belongs to class $i$. Then \n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{N} log\\sigma(W[i,:]D[:,j])\n",
    "$$\n",
    "\n",
    "In other words, the log-likelihood has a contribution equal to the log of the\n",
    "probability that the model assigns to point j belonging to its actual class.\n",
    "We want to maximize this.\n",
    "\n",
    "To apply gradient descent we need the gradient of the $\\log\\sigma(Wx)$.\n",
    "Recall that\n",
    "$$\n",
    "\\sigma(W[i,:]D[:,j]) = \\frac{e^{W[i,:]D[:,j]}}{Z}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "Z = \\sum_{i=1}^{k} e^{W[i,:]D[:,j]}.\n",
    "$$\n",
    "\n",
    "Taking the logarithmic derivative of $\\partial_i=\\partial/\\partial W[i,:]$\n",
    "gives\n",
    "$$\n",
    "\\partial_i \\log\\sigma(W[j,:]D[:,k]] = -\\log\\partial_i(Z)\n",
    "$$\n",
    "if $j\\not=i$ and\n",
    "$$\n",
    "\\partial_i\\log\\sigma(W[i,:]D[:,k]) = D[:,k]-\\log\\partial_i(Z)\n",
    "$$\n",
    "\n",
    "Using the definition of $Z$ we get\n",
    "$$\n",
    "\\partial_i\\log(Z) = D[:,k]\\sigma(W[i,:]D[:,k])\n",
    "$$\n",
    "\n",
    "\n",
    "To make sense of these formulas, each term in the sum making up the log likelihood\n",
    " -- that is, each data point -- contributes to the gradient.  That contribution\n",
    " is $\\log\\sigma(W[i,:]D[:,j])$ where $i$ is the class to which the point belongs\n",
    " The contribution of this term to the gradient in row i is the a multiple\n",
    " of the data point:\n",
    " $$\n",
    " \\partial_i = D[:,k](1-\\sigma(W[i,:]D[:,k]))\n",
    " $$\n",
    " while in other rows the multiple is:\n",
    " $$\n",
    " \\partial_j = -D[:,k](\\sigma(W[i,:]D[:,k]))\n",
    " $$\n",
    " \n",
    " Since the probabilities $\\sigma(W[i,:]D[:,k])$ are between $0$ and $1$,\n",
    " this means that if we follow the gradient the weights in the ith row will \n",
    " be increased a bit while the weights in all the other rows will be\n",
    " decreased a bit in the direction of the jth data point if that point belongs to class i.\n",
    " \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
