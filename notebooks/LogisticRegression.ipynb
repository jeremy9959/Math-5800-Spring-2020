{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of:\n",
    "\n",
    "- A collection D of N d-dimensional column vectors x (so an \n",
    "    (d,N) data matrix) \n",
    "    \n",
    "- A label for each column vector between 0 and k-1 which we represent as an \n",
    "    (k,N)\n",
    "    matrix $L$ such that the jth column has a 1 in the ith row if D[:,j] is in class i,\n",
    "    and zeroes elsewhere in that column.\n",
    "\n",
    "The logistic regression model has for parameters a matrix W of size (k,d)\n",
    "and a bias vector b of\n",
    "size (k,1). To simplify the notation, let's add a dimension to our column\n",
    "vectors and set every last value to 1; then we can treat the biases\n",
    "as part of our matrices W.  From now on our dimension d includes this extra\n",
    "dimension.\n",
    "\n",
    "\n",
    "Let $\\sigma$ be the softmax function.  The model assigns the multinomial probabilities\n",
    "$\\sigma(Wx)$ to x and we wish to maximize the likelihood of the data given W.  Note that $\\sigma(Wx)$  is a (k,1) column vector. \n",
    "\n",
    "The relevant loss function is the log-likelihood of the model.  Suppose\n",
    "that the point $D[:,j]$ belongs to class $i$. Then \n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{N} log\\sigma(W[i,:]D[:,j])\n",
    "$$\n",
    "\n",
    "In other words, the log-likelihood has a contribution equal to the log of the\n",
    "probability that the model assigns to point j belonging to its actual class.\n",
    "We want to maximize this.\n",
    "\n",
    "To apply gradient descent we need the gradient of the $\\log\\sigma(Wx)$.\n",
    "Recall that\n",
    "$$\n",
    "(\\sigma(WD[:,j]))_i = \\frac{e^{W[i,:]D[:,j]}}{Z}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "Z = \\sum_{i=1}^{k} e^{W[i,:]D[:,j]}.\n",
    "$$\n",
    "\n",
    "Taking the logarithmic derivative of $\\partial_i=\\partial/\\partial W[i,:]$\n",
    "gives\n",
    "$$\n",
    "\\partial_i (\\log\\sigma(WD[:,k]])_j = -\\log\\partial_i(Z)\n",
    "$$\n",
    "if $j\\not=i$ and\n",
    "$$\n",
    "\\partial_i\\log\\sigma(WD[:,k])_i = D[:,k]-\\log\\partial_i(Z)\n",
    "$$\n",
    "\n",
    "Using the definition of $Z$ we get\n",
    "$$\n",
    "\\partial_i\\log(Z) = D[:,k](\\sigma(WD[:,k])_i\n",
    "$$\n",
    "\n",
    "\n",
    "To make sense of these formulas, each term in the sum making up the log likelihood\n",
    " -- that is, each data point -- contributes to the gradient.  That contribution\n",
    " is $\\log\\sigma(WD[:,j])_i$ where $i$ is the class to which the point belongs\n",
    " The contribution of this term to the gradient in row i is the a multiple\n",
    " of the data point:\n",
    " $$\n",
    " \\partial_i = D[:,k](1-(\\sigma(WD[:,k])_i)\n",
    " $$\n",
    " while in other rows the multiple is:\n",
    " $$\n",
    " \\partial_j = -D[:,k](\\sigma(WD[:,k])_i)\n",
    " $$\n",
    " \n",
    " Since the probabilities $\\sigma(WD[:,k])_i$ are between $0$ and $1$,\n",
    " this means that if we follow the gradient the weights in the ith row will \n",
    " be increased a bit while the weights in all the other rows will be\n",
    " decreased a bit in the direction of the jth data point if that point belongs to class i.\n",
    " \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import normal\n",
    "from numpy.random import shuffle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make N points in dim dimensions chosen from two normals at diagonal means 1 and -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(npts=100,dim=2):\n",
    "    N = npts\n",
    "    dim = 2\n",
    "    A = normal(1,size=dim*N).reshape(dim,N)\n",
    "    LA = np.concatenate([np.ones(N).reshape(1,N),np.zeros(N).reshape(1,N)],axis=0)\n",
    "    B = normal(-1,size=dim*N).reshape(dim,N)\n",
    "    LB = np.concatenate([np.zeros(N).reshape(1,N),np.ones(N).reshape(1,N)],axis=0)\n",
    "    D = np.concatenate([A,B],axis=1)\n",
    "    L = np.concatenate([LA,LB],axis=1)\n",
    "    D=np.concatenate([D,np.ones(2*N).reshape(1,2*N)])\n",
    "    W=np.random.normal(size=2*(dim+1)).reshape(2,dim+1)\n",
    "    return D, L, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, L, W = setup(npts=100,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(W,D):\n",
    "    n = np.exp(np.dot(W,D))\n",
    "    Z = n.sum(axis=0)\n",
    "    return n/Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(W,D,L):\n",
    "    return (np.log(sigma(W,D))*L).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(W,D,L):\n",
    "    dim = D.shape[0]\n",
    "    npts = D.shape[1]\n",
    "    E=0\n",
    "    for n in range(npts):\n",
    "        if L[0,n]==1:\n",
    "            E += np.concatenate([(1-sigma(W,D)[0,n])*D[:,n].reshape(1,dim),-sigma(W,D)[1,n]*D[:,n].reshape(1,dim)])       \n",
    "        else:\n",
    "            E += np.concatenate([-sigma(W,D)[0,n]*D[:,n].reshape(1,dim),(1-sigma(W,D)[1,n])*D[:,n].reshape(1,dim)])\n",
    "    return E/np.linalg.norm(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(D, L, W, max_iter=10000,threshold=.001):\n",
    "    npts = D.shape[1]\n",
    "    e = 0\n",
    "    eL = []\n",
    "    for i in tqdm(range(max_iter)):\n",
    "        W = W + .001*gradient(W,D,L)\n",
    "        enew = loss(W,D,L) \n",
    "        if np.abs(e-enew)<threshold:\n",
    "            print(\"stopping after {} iterations with loss {}\".format(i,enew))\n",
    "            return W, eL\n",
    "        e = enew\n",
    "        eL.append(e)\n",
    "    print(\"max iterations reached, probably no convergence\")\n",
    "    return W, eL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(D, L, W):\n",
    "    npts = D.shape[1]\n",
    "    mistakes = 0\n",
    "    for i in range(npts):\n",
    "        if (np.argmax(sigma(W,D)[:,i])==0) != (L[0,i]==1):\n",
    "            mistakes +=1\n",
    "    return mistakes/npts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, L, W = setup(200,2)\n",
    "W, eL = fit(D,L,W)\n",
    "print('error rate is {:.2f} percent'.format(100*error_rate(D,L,W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.plot(range(len(eL)),eL)\n",
    "_=plt.suptitle('log loss behavior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.figure(figsize=(10,10))\n",
    "plt.style.use('ggplot')\n",
    "P = sigma(W,D)\n",
    "_=plt.scatter(x=D[0,:],y=D[1,:],c=L[0,:])\n",
    "B = (W[0,:]-W[1,:])\n",
    "B = B/B[1]\n",
    "x = np.linspace(-4,4,100)\n",
    "plt.ylim([-4,4])\n",
    "y = -B[0]*x-B[2]\n",
    "_=plt.plot(x,y)\n",
    "_=plt.title(\"Logistic Regression with slope {:.2f} and intercept {:.2f}\".format(-B[0],-B[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SK = LogisticRegression().fit(D[:-1,:].T,np.concatenate([[1 for i in range(D.shape[1]//2)],[0 for i in range(D.shape[1]//2)]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = SK.coef_.T\n",
    "b = SK.intercept_\n",
    "-C[0]/C[1],-b/C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
